---
hide_table_of_contents: true
hide_title: true
custom_edit_url: null
---

import CenterLayout from "/src/components/CenterLayout"
import GitHub from "/src/components/GitHub"

<head>
  <link rel="canonical" href="https://github.com/gruntwork-io/knowledge-base/discussions/681" />
</head>

<CenterLayout>
<span className="searchCategory">Knowledge Base</span>
<h1>I received an email from AWS regarding shared NAT Gateways, what should I do?</h1>
<GitHub discussion={{"id":"D_kwDOF8slf84AS4jD","number":681,"author":{"login":"arsci"},"title":"I received an email from AWS regarding shared NAT Gateways, what should I do?","body":"\nI received the following email from AWS regarding NAT Gateways. As a Reference Architecture customer and Gruntwork VPC module user I would like to know if there is anything I need to do\r\n\r\n\r\n> We have observed that your Amazon VPC resources are using a shared NAT Gateway across multiple Availability Zones (AZ). To ensure high availability and minimize inter-AZ data transfer costs, we recommend utilizing separate NAT Gateways in each AZ and routing traffic locally within the same AZ.\r\nEach NAT Gateway operates within a designated AZ and is built with redundancy in that zone only. As a result, if the NAT Gateway or AZ experiences failure, resources utilizing that NAT Gateway in other AZ(s) also get impacted. Additionally, routing traffic from one AZ to a NAT Gateway in a different AZ incurs additional inter-AZ data transfer charges. We recommend choosing a maintenance window for architecture changes in your Amazon VPC.\r\nThe following is a list of your VPCs and NAT Gateways that are shared across AZ(s), in the format: ‘VPC | NAT Gateway’:\r\nvpc-xxxxxx | nat-xxxxxx\r\nPlease refer to the AWS public documentation on how to create a NAT Gateway [1], and how to configure routes for different NAT Gateway use cases [2].\r\nShould you have any questions or concerns, please reach out to the AWS Support team [3].\r\n[1] https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-working-with\r\n[2] https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-scenarios.html\r\n[3] https://aws.amazon.com/support\n\n---\n\n<ins datetime=\"2023-03-10T19:52:39Z\">\n  <p><a href=\"https://support.gruntwork.io/hc/requests/109981\">Tracked in ticket #109981</a></p>\n</ins>\n","bodyHTML":"<p dir=\"auto\">I received the following email from AWS regarding NAT Gateways. As a Reference Architecture customer and Gruntwork VPC module user I would like to know if there is anything I need to do</p>\n<blockquote>\n<p dir=\"auto\">We have observed that your Amazon VPC resources are using a shared NAT Gateway across multiple Availability Zones (AZ). To ensure high availability and minimize inter-AZ data transfer costs, we recommend utilizing separate NAT Gateways in each AZ and routing traffic locally within the same AZ.<br>\nEach NAT Gateway operates within a designated AZ and is built with redundancy in that zone only. As a result, if the NAT Gateway or AZ experiences failure, resources utilizing that NAT Gateway in other AZ(s) also get impacted. Additionally, routing traffic from one AZ to a NAT Gateway in a different AZ incurs additional inter-AZ data transfer charges. We recommend choosing a maintenance window for architecture changes in your Amazon VPC.<br>\nThe following is a list of your VPCs and NAT Gateways that are shared across AZ(s), in the format: ‘VPC | NAT Gateway’:<br>\nvpc-xxxxxx | nat-xxxxxx<br>\nPlease refer to the AWS public documentation on how to create a NAT Gateway [1], and how to configure routes for different NAT Gateway use cases [2].<br>\nShould you have any questions or concerns, please reach out to the AWS Support team [3].<br>\n[1] <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-working-with\" rel=\"nofollow\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-working-with</a><br>\n[2] <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-scenarios.html\" rel=\"nofollow\">https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-scenarios.html</a><br>\n[3] <a href=\"https://aws.amazon.com/support\" rel=\"nofollow\">https://aws.amazon.com/support</a></p>\n</blockquote>\n<hr>\n<ins datetime=\"2023-03-10T19:52:39Z\">\n  <p dir=\"auto\"><a href=\"https://support.gruntwork.io/hc/requests/109981\" rel=\"nofollow\">Tracked in ticket #109981</a></p>\n</ins>","answer":{"body":"AWS recently sent an email notification about using shared NAT Gateways in VPCs and recommended a separate NAT Gateway for each availability zone (AZ) to ensure high availability and to minimize inter-AZ data transfer costs.\r\n\r\n### **How serious is this?** \r\n\r\nFirst, there is no security implication, underlying module bug, or critical status.\r\n\r\nIf you received the AWS notification, AWS identified an issue where your network configuration could be made more resilient, and we agree with their recommendation. The main downside of not following this recommendation is the possibility that your apps might be unavailable if just one AZ fails, versus multiple AZs failing before your app becomes unavailable. The net effect of this change will be more AWS costs but also more resilience.\r\n\r\n### Gruntwork Reference Architecture Customers\r\n\r\nWe found a sub-optimal configuration in the default Reference Architecture where the `vpc-app` module set the `num_nat_gateways` variable to 1, even for a production environment. This should have been set to 3.\r\n\r\nAs a result, we recommend you update your reference architecture configuration prod environment to use multiple NAT Gateways for the `vpc-app`.\r\n\r\nTo update this configuration, modify the `num_nat_gateways` input in your Terragrunt configuration to deploy the desired number of NAT Gateways for your application VPC (at least 3 NAT Gateways, or 1-per AZ is recommended). The Gruntwork VPC Module will automatically distribute the requested number of NAT Gateways across available AZs and configure your private app subnet route tables accordingly. (Always test and validate any changes before deploying to production environments!) \r\n\r\nThe module itself does not have a bug. The mistake was deploying a Reference Architecture with the wrong value for `num_nat_gateways`. While we agree with Amazon’s recommendation and 1 NAT Gateway per AZ is a best practice, the main downside of not doing this is the possibility that your apps might be unavailable if just one of the AZs fails, versus multiple AZs. The net effect of this change will be more cost but also more resilience.\r\n\r\nFind the relevant application VPC Terragrunt configuration in the inputs block in `_envcommon/networking/vpc-app.hcl`.\r\n\r\nIf you also want to add additional NAT Gateways to your management VPC, find the relevant Terragrunt configuration in the inputs block in `_envcommon/mgmt/vpc-mgmt.hcl`.\r\n\r\nWe tested adding additional NAT Gateways and found that Terraform uses the ReplaceRoute API to update the route table rules. We found that there was no observable downtime or network interruptions while the route table rules were being updated. As AWS mentioned in the email, choose a maintenance window for architecture changes in your VPC. Always test and validate changes in lower environments before deploying to production. \r\n\r\n**Also see:**\r\n* [Gruntwork Docs: Making changes to your infrastructure: Terragrunt](https://docs.gruntwork.io/reference/services/intro/make-changes-to-your-infrastructure/#making-changes-to-terragrunt-code)\r\n* [Gruntwork Docs: `num_nat_gateways`](https://docs.gruntwork.io/reference/modules/terraform-aws-vpc/vpc-app/#num_nat_gateways)\r\n\r\n### Gruntwork VPC Module Users\r\n\r\nFor users who want to configure additional NAT Gateways in a VPC created with the Gruntwork VPC Module, specify the number of gateways you'd like to create with the `num_nat_gateways` input. The VPC module will automatically distribute your requested NAT Gateways across available AZs and configure your private subnet route table rules accordingly.\r\n\r\nWe tested adding additional NAT Gateways and found that Terraform uses the ReplaceRoute API to update the route table rules. We found that there was no observable downtime or network interruptions while the route table rules were being updated. As AWS mentioned in the email, choose a maintenance window for architecture changes in your VPC. Always test and validate changes in lower environments before deploying to production. \r\n\r\n**Also see:** \r\n* [Gruntwork Docs: Making changes to your infrastructure: Terraform](https://docs.gruntwork.io/reference/services/intro/make-changes-to-your-infrastructure/#making-changes-to-vanilla-terraform-code)\r\n* [Gruntwork Docs: `num_nat_gateways`](https://docs.gruntwork.io/reference/modules/terraform-aws-vpc/vpc-app/#num_nat_gateways)\r\n","bodyHTML":"<p dir=\"auto\">AWS recently sent an email notification about using shared NAT Gateways in VPCs and recommended a separate NAT Gateway for each availability zone (AZ) to ensure high availability and to minimize inter-AZ data transfer costs.</p>\n<h3 dir=\"auto\"><strong>How serious is this?</strong></h3>\n<p dir=\"auto\">First, there is no security implication, underlying module bug, or critical status.</p>\n<p dir=\"auto\">If you received the AWS notification, AWS identified an issue where your network configuration could be made more resilient, and we agree with their recommendation. The main downside of not following this recommendation is the possibility that your apps might be unavailable if just one AZ fails, versus multiple AZs failing before your app becomes unavailable. The net effect of this change will be more AWS costs but also more resilience.</p>\n<h3 dir=\"auto\">Gruntwork Reference Architecture Customers</h3>\n<p dir=\"auto\">We found a sub-optimal configuration in the default Reference Architecture where the <code class=\"notranslate\">vpc-app</code> module set the <code class=\"notranslate\">num_nat_gateways</code> variable to 1, even for a production environment. This should have been set to 3.</p>\n<p dir=\"auto\">As a result, we recommend you update your reference architecture configuration prod environment to use multiple NAT Gateways for the <code class=\"notranslate\">vpc-app</code>.</p>\n<p dir=\"auto\">To update this configuration, modify the <code class=\"notranslate\">num_nat_gateways</code> input in your Terragrunt configuration to deploy the desired number of NAT Gateways for your application VPC (at least 3 NAT Gateways, or 1-per AZ is recommended). The Gruntwork VPC Module will automatically distribute the requested number of NAT Gateways across available AZs and configure your private app subnet route tables accordingly. (Always test and validate any changes before deploying to production environments!)</p>\n<p dir=\"auto\">The module itself does not have a bug. The mistake was deploying a Reference Architecture with the wrong value for <code class=\"notranslate\">num_nat_gateways</code>. While we agree with Amazon’s recommendation and 1 NAT Gateway per AZ is a best practice, the main downside of not doing this is the possibility that your apps might be unavailable if just one of the AZs fails, versus multiple AZs. The net effect of this change will be more cost but also more resilience.</p>\n<p dir=\"auto\">Find the relevant application VPC Terragrunt configuration in the inputs block in <code class=\"notranslate\">_envcommon/networking/vpc-app.hcl</code>.</p>\n<p dir=\"auto\">If you also want to add additional NAT Gateways to your management VPC, find the relevant Terragrunt configuration in the inputs block in <code class=\"notranslate\">_envcommon/mgmt/vpc-mgmt.hcl</code>.</p>\n<p dir=\"auto\">We tested adding additional NAT Gateways and found that Terraform uses the ReplaceRoute API to update the route table rules. We found that there was no observable downtime or network interruptions while the route table rules were being updated. As AWS mentioned in the email, choose a maintenance window for architecture changes in your VPC. Always test and validate changes in lower environments before deploying to production.</p>\n<p dir=\"auto\"><strong>Also see:</strong></p>\n<ul dir=\"auto\">\n<li><a href=\"https://docs.gruntwork.io/reference/services/intro/make-changes-to-your-infrastructure/#making-changes-to-terragrunt-code\" rel=\"nofollow\">Gruntwork Docs: Making changes to your infrastructure: Terragrunt</a></li>\n<li><a href=\"https://docs.gruntwork.io/reference/modules/terraform-aws-vpc/vpc-app/#num_nat_gateways\" rel=\"nofollow\">Gruntwork Docs: <code class=\"notranslate\">num_nat_gateways</code></a></li>\n</ul>\n<h3 dir=\"auto\">Gruntwork VPC Module Users</h3>\n<p dir=\"auto\">For users who want to configure additional NAT Gateways in a VPC created with the Gruntwork VPC Module, specify the number of gateways you'd like to create with the <code class=\"notranslate\">num_nat_gateways</code> input. The VPC module will automatically distribute your requested NAT Gateways across available AZs and configure your private subnet route table rules accordingly.</p>\n<p dir=\"auto\">We tested adding additional NAT Gateways and found that Terraform uses the ReplaceRoute API to update the route table rules. We found that there was no observable downtime or network interruptions while the route table rules were being updated. As AWS mentioned in the email, choose a maintenance window for architecture changes in your VPC. Always test and validate changes in lower environments before deploying to production.</p>\n<p dir=\"auto\"><strong>Also see:</strong></p>\n<ul dir=\"auto\">\n<li><a href=\"https://docs.gruntwork.io/reference/services/intro/make-changes-to-your-infrastructure/#making-changes-to-vanilla-terraform-code\" rel=\"nofollow\">Gruntwork Docs: Making changes to your infrastructure: Terraform</a></li>\n<li><a href=\"https://docs.gruntwork.io/reference/modules/terraform-aws-vpc/vpc-app/#num_nat_gateways\" rel=\"nofollow\">Gruntwork Docs: <code class=\"notranslate\">num_nat_gateways</code></a></li>\n</ul>"}}} />

</CenterLayout>
  

<!-- ##DOCS-SOURCER-START
{
  "sourcePlugin": "github-discussions",
  "hash": "94ac61853e975b676ab8dac775f2f4b7"
}
##DOCS-SOURCER-END -->
