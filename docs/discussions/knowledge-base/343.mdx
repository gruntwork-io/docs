---
hide_table_of_contents: true
hide_title: true
custom_edit_url: null
---

import CenterLayout from "/src/components/CenterLayout"
import GitHub from "/src/components/GitHub"

<head>
  <link rel="canonical" href="https://github.com/gruntwork-io/knowledge-base/discussions/343" />
</head>

<CenterLayout>
<span className="searchCategory">Knowledge Base</span>
<h1>Reference Architecture: EKS Fargate nodes down or in NotReady or Unknown status. 503</h1>
<GitHub discussion={{"id":"D_kwDOF8slf84APPIi","number":343,"author":{"login":"zackproser"},"title":"Reference Architecture: EKS Fargate nodes down or in NotReady or Unknown status. 503","body":"A customer asked: \r\n> I have a EKS Fargate-backed cluster that is running the AWS Sample App that was delivered with my Reference Architecture. One or more of the app environments is no longer healthy and is now returning a 503. How can I debug and fix this? \r\n\r\n","bodyHTML":"<p dir=\"auto\">A customer asked:</p>\n<blockquote>\n<p dir=\"auto\">I have a EKS Fargate-backed cluster that is running the AWS Sample App that was delivered with my Reference Architecture. One or more of the app environments is no longer healthy and is now returning a 503. How can I debug and fix this?</p>\n</blockquote>","answer":{"body":"If you're seeing the following screen for one of your app accounts (dev, stage, prod) and if you're running EKS, this answer will help you diagnose and fix the underlying problem:\r\n\r\n![503](https://user-images.githubusercontent.com/1769996/162269004-cbe44c64-5857-4838-ae40-7455b1684578.png)\r\n\r\n# 1. Get access to your EKS cluster\r\nOpen the `docs/` folder in your infrastructure-live repository and find the document named `03-deploy-apps.md`. In this document, there is a section that explains how to gain access to your EKS cluster. \r\n\r\nFirst, you will need to configure your own access to your Reference Architecture accounts. If you have not already done so, visit the `docs/02-authenticate.md` file and follow the steps to set up your access to your accounts. Ensure you complete the section titled **Authenticate to AWS via the CLI**.\r\n\r\nNote there is a section in this guide where we've already generated a valid `~/.aws/config` file for you to use alongside [aws-vault](https://github.com/99designs/aws-vault). For the remainder of this guide, we'll assume you configured access via `aws-vault`. \r\n\r\nOnce you have successfully configured your CLI access to your Reference Architecture, you can `cd` into the unhealthy environment's EKS cluster folder. Let's assume your `prod` account is unhealthy. From the root of your infrastructure-live repository, `cd` into `prod/<your-region>/prod/services/eks-cluster`. \r\n\r\nFrom here, first authenticate to your correct prod account, and then run `terragrunt output` in order to discover the ARN of the EKS cluster, like so: \r\n`aws-vault exec <your-prod-account-profile-name> -- terragrunt output`\r\n\r\nIn your output you should find a similar entry to the following:\r\n\r\n`eks_cluster_arn = \"arn:aws:eks:us-east-2:226340335990:cluster/example-prod\"`\r\n\r\nCopy this ARN to your clipboard. \r\n\r\nEnsure that you have `kubergrunt` installed locally. If you don't - you can [get kubergrunt here](https://github.com/gruntwork-io/kubergrunt#installation).\r\n\r\nNext, run the following command to configure access to your EKS cluster via kubectl: \r\n\r\n`kubergrunt eks configure --eks-cluster-arn ARN_OF_EKS_CLUSTER_THAT_YOU_COPIED`\r\n\r\nYou should see output similar to the following: \r\n\r\n```bash \r\n[] INFO[2022-04-07T12:22:42-04:00] Retrieving details for EKS cluster arn:aws:eks:us-east-2:226340335990:cluster/example-prod  name=kubergrunt\r\n[] INFO[2022-04-07T12:22:42-04:00] Detected cluster deployed in region us-east-2  name=kubergrunt\r\n[] INFO[2022-04-07T12:22:43-04:00] Successfully retrieved EKS cluster details    name=kubergrunt\r\n[] INFO[2022-04-07T12:22:43-04:00] Loading kubectl config /home/<your-machine>/.kube/config.  name=kubergrunt\r\n[] INFO[2022-04-07T12:22:43-04:00] Successfully loaded and parsed kubectl config.  name=kubergrunt\r\n```\r\nYou are now able to interact with your EKS cluster directly, with `kubectl`.\r\n\r\n# 2. Inspect your EKS cluster with `kubectl`\r\n\r\nRun `aws-vault exec <your-prod-aws-vault-profile> -- kubectl get deployments -n applications`\r\n\r\nYou'll see output like the following. In this example case, both our frontend and backend deployments are unhealthy. \r\n\r\n```\r\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\r\nsample-app-backend-prod    0/1     1            0           42m\r\nsample-app-frontend-prod   0/1     1            0           42m\r\n```\r\nWe can look for more information as to why by describing our deployments next. \r\n\r\nRun `aws-vault exec <your-prod-aws-vault-profile> -- kubectl describe deployments sample-app-backend-prod -n applications`\r\n \r\n```\r\nName:                   sample-app-backend-prod\r\nNamespace:              applications\r\nCreationTimestamp:      Thu, 07 Apr 2022 11:41:37 -0400\r\nLabels:                 app.kubernetes.io/instance=sample-app-backend-prod\r\n                        app.kubernetes.io/managed-by=Helm\r\n                        app.kubernetes.io/name=sample-app-backend-prod\r\n                        helm.sh/chart=k8s-service-v0.2.12\r\nAnnotations:            deployment.kubernetes.io/revision: 1\r\n                        meta.helm.sh/release-name: sample-app-backend-prod\r\n                        meta.helm.sh/release-namespace: applications\r\nSelector:               app.kubernetes.io/instance=sample-app-backend-prod,app.kubernetes.io/name=sample-app-backend-prod,gruntwork.io/deployment-type=main\r\nReplicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\r\nStrategyType:           RollingUpdate\r\nMinReadySeconds:        0\r\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\r\nPod Template:\r\n  Labels:           app.kubernetes.io/instance=sample-app-backend-prod\r\n                    app.kubernetes.io/name=sample-app-backend-prod\r\n                    gruntwork.io/deployment-type=main\r\n  Service Account:  gruntwork-sample-app-backend\r\n  Containers:\r\n   sample-app-backend-prod:\r\n    Image:       gruntwork/aws-sample-app:v0.0.4\r\n    Ports:       8443/TCP, 8443/TCP, 8443/TCP\r\n    Host Ports:  0/TCP, 0/TCP, 0/TCP\r\n    Liveness:    http-get https://:8443/health delay=15s timeout=1s period=30s #success=1 #failure=3\r\n    Readiness:   http-get https://:8443/greeting delay=15s timeout=1s period=30s #success=1 #failure=3\r\n    Environment:\r\n      CONFIG_APP_ENVIRONMENT_NAME:            prod\r\n      CONFIG_APP_NAME:                        backend\r\n      CONFIG_DATABASE_HOST:                   database\r\n      CONFIG_DATABASE_POOL_SIZE:              10\r\n      CONFIG_DATABASE_RUN_SCHEMA_MIGRATIONS:  true\r\n      CONFIG_SECRETS_DIR:                     /mnt/secrets\r\n      CONFIG_SECRETS_SECRETS_MANAGER_DB_ID:   arn:aws:secretsmanager:us-east-2:226340335990:secret:RDSDBConfig-hECb4Z\r\n      CONFIG_SECRETS_SECRETS_MANAGER_REGION:  us-east-2\r\n      CONFIG_SECRETS_SECRETS_MANAGER_TLS_ID:  arn:aws:secretsmanager:us-east-2:226340335990:secret:SampleAppBackEndCA-ALpZCe\r\n    Mounts:\r\n      /mnt/secrets/backend-secrets from secrets-manager-scratch (rw)\r\n  Volumes:\r\n   secrets-manager-scratch:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\nConditions:\r\n  Type           Status  Reason\r\n  ----           ------  ------\r\n  Progressing    True    NewReplicaSetAvailable\r\n  Available      False   MinimumReplicasUnavailable\r\nOldReplicaSets:  <none>\r\nNewReplicaSet:   sample-app-backend-prod-577fc88dbd (1/1 replicas created)\r\nEvents:\r\n  Type    Reason             Age   From                   Message\r\n  ----    ------             ----  ----                   -------\r\n  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set sample-app-backend-prod-577fc88dbd to 1\r\n \r\n```\r\n\r\nThis confirms that our app is not available because we've not reached the desired number of replicas. \r\n\r\nLet's step one level deeper and inpsect the nodes making up our EKS cluster, keeping in mind that, since our Ref Arch was just deployed, these nodes may be backed by Fargate: \r\n\r\n# 3. Inspect your EKS cluster nodes\r\n\r\nRun `aws-vault exec <your-prod-aws-vault-profile> -- kubectl get nodes`\r\n\r\nYou'll see output similar to the following: \r\n\r\n```\r\nNAME                                                   STATUS     ROLES    AGE     VERSION\r\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready      <none>   76m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready      <none>   76m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-103-47.us-east-2.compute.internal     Ready      <none>   79m     v1.21.2-eks-06eac09\r\nip-11-103-103-113.us-east-2.compute.internal           NotReady   <none>   79m     v1.21.5-eks-9017834\r\nip-11-102-83-84.us-east-2.compute.internal             NotReady   <none>   79m     v1.21.5-eks-9017834\r\nip-11-103-92-3.us-east-2.compute.internal              Ready      <none>   2m17s   v1.21.5-eks-9017834\r\n``` \r\nNote the two unhealthy nodes, with status `NotReady`. You could also verify this in the AWS web console. In this particular case the nodes were unhealthy due to an underlying hardware failure on AWS's side. This leads to the kubelet on the node being unable to report its status, including its memory and CPU usage. When this happens, EKS eventually marks the Fargate node's status as `unknown`, leading to issues scheduling new healthy pods on the unknown nodes. \r\n\r\nTherefore, we can drain and delete these unhealthy nodes. This will allow EKS to detect their becoming unavailable and automatically reconcile them by launching new, hopefully healthy, nodes to replace them!\r\n\r\n# 4. Drain and delete unhealthy nodes\r\n\r\nFirst, we'll want to drain the nodes, which safely evicts all pods from the node and makes the node ready for maintenance or deletion: \r\n\r\nRun `aws-vault exec <your-prod-aws-vault-profile> -- kubectl drain ip-11-103-103-113.us-east-2.compute.internal`\r\n\r\nNext, delete the node. \r\n\r\nRun `aws-vault exec <your-prod-aws-vault-profile> -- kubectl delete ip-11-103-103-113.us-east-2.compute.internal`\r\n\r\nRepeat the above two steps of draining and deleting the node for the second unhealthy node. \r\n\r\nOnce this is complete, EKS should kick in and provision two new healthy nodes to compensate for the unhealthy ones you just cleaned up. \r\n\r\nYou can confirm this by running `aws-vault exec <your-prod-aws-vault-profile> -- kubectl get nodes`. You should see output similar to the following: \r\n\r\n```\r\nNAME                                                   STATUS   ROLES    AGE     VERSION\r\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready    <none>   81m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready    <none>   81m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-87-47.us-east-2.compute.internal     Ready    <none>   84m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-94-175.us-east-2.compute.internal    Ready    <none>   55m     v1.21.2-eks-06eac09\r\nfargate-ip-11-103-95-219.us-east-2.compute.internal    Ready    <none>   84m     v1.21.2-eks-06eac09\r\nip-10-102-92-3.us-east-2.compute.internal              Ready    <none>   7m49s   v1.21.5-eks-9017834\r\n```\r\n\r\nWith new healthy nodes available, EKS should have been able to schedule your sample app pods on these new nodes, so things should be starting to recover. \r\n\r\nAt this point, you can re-check your [AWS Sample App](https://github.com/gruntwork-io/aws-sample-app) and it should be healthy again!\r\n\r\n![sample-app-healthy](https://user-images.githubusercontent.com/1769996/162277046-2a4da02f-41a4-4b94-9ff6-a3ec44f3ab6a.png)\r\n","bodyHTML":"<p dir=\"auto\">If you're seeing the following screen for one of your app accounts (dev, stage, prod) and if you're running EKS, this answer will help you diagnose and fix the underlying problem:</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://user-images.githubusercontent.com/1769996/162269004-cbe44c64-5857-4838-ae40-7455b1684578.png\"><img src=\"https://user-images.githubusercontent.com/1769996/162269004-cbe44c64-5857-4838-ae40-7455b1684578.png\" alt=\"503\" style=\"max-width: 100%;\"></a></p>\n<h1 dir=\"auto\">1. Get access to your EKS cluster</h1>\n<p dir=\"auto\">Open the <code class=\"notranslate\">docs/</code> folder in your infrastructure-live repository and find the document named <code class=\"notranslate\">03-deploy-apps.md</code>. In this document, there is a section that explains how to gain access to your EKS cluster.</p>\n<p dir=\"auto\">First, you will need to configure your own access to your Reference Architecture accounts. If you have not already done so, visit the <code class=\"notranslate\">docs/02-authenticate.md</code> file and follow the steps to set up your access to your accounts. Ensure you complete the section titled <strong>Authenticate to AWS via the CLI</strong>.</p>\n<p dir=\"auto\">Note there is a section in this guide where we've already generated a valid <code class=\"notranslate\">~/.aws/config</code> file for you to use alongside <a href=\"https://github.com/99designs/aws-vault\">aws-vault</a>. For the remainder of this guide, we'll assume you configured access via <code class=\"notranslate\">aws-vault</code>.</p>\n<p dir=\"auto\">Once you have successfully configured your CLI access to your Reference Architecture, you can <code class=\"notranslate\">cd</code> into the unhealthy environment's EKS cluster folder. Let's assume your <code class=\"notranslate\">prod</code> account is unhealthy. From the root of your infrastructure-live repository, <code class=\"notranslate\">cd</code> into <code class=\"notranslate\">prod/&lt;your-region&gt;/prod/services/eks-cluster</code>.</p>\n<p dir=\"auto\">From here, first authenticate to your correct prod account, and then run <code class=\"notranslate\">terragrunt output</code> in order to discover the ARN of the EKS cluster, like so:<br>\n<code class=\"notranslate\">aws-vault exec &lt;your-prod-account-profile-name&gt; -- terragrunt output</code></p>\n<p dir=\"auto\">In your output you should find a similar entry to the following:</p>\n<p dir=\"auto\"><code class=\"notranslate\">eks_cluster_arn = \"arn:aws:eks:us-east-2:226340335990:cluster/example-prod\"</code></p>\n<p dir=\"auto\">Copy this ARN to your clipboard.</p>\n<p dir=\"auto\">Ensure that you have <code class=\"notranslate\">kubergrunt</code> installed locally. If you don't - you can <a href=\"https://github.com/gruntwork-io/kubergrunt#installation\">get kubergrunt here</a>.</p>\n<p dir=\"auto\">Next, run the following command to configure access to your EKS cluster via kubectl:</p>\n<p dir=\"auto\"><code class=\"notranslate\">kubergrunt eks configure --eks-cluster-arn ARN_OF_EKS_CLUSTER_THAT_YOU_COPIED</code></p>\n<p dir=\"auto\">You should see output similar to the following:</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"[] INFO[2022-04-07T12:22:42-04:00] Retrieving details for EKS cluster arn:aws:eks:us-east-2:226340335990:cluster/example-prod  name=kubergrunt\n[] INFO[2022-04-07T12:22:42-04:00] Detected cluster deployed in region us-east-2  name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Successfully retrieved EKS cluster details    name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Loading kubectl config /home/&lt;your-machine&gt;/.kube/config.  name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Successfully loaded and parsed kubectl config.  name=kubergrunt\"><pre class=\"notranslate\">[] INFO[2022-04-07T12:22:42-04:00] Retrieving details <span class=\"pl-k\">for</span> EKS cluster arn:aws:eks:us-east-2:226340335990:cluster/example-prod  name=kubergrunt\n[] INFO[2022-04-07T12:22:42-04:00] Detected cluster deployed <span class=\"pl-k\">in</span> region us-east-2  name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Successfully retrieved EKS cluster details    name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Loading kubectl config /home/<span class=\"pl-k\">&lt;</span>your-machine<span class=\"pl-k\">&gt;</span>/.kube/config.  name=kubergrunt\n[] INFO[2022-04-07T12:22:43-04:00] Successfully loaded and parsed kubectl config.  name=kubergrunt</pre></div>\n<p dir=\"auto\">You are now able to interact with your EKS cluster directly, with <code class=\"notranslate\">kubectl</code>.</p>\n<h1 dir=\"auto\">2. Inspect your EKS cluster with <code class=\"notranslate\">kubectl</code></h1>\n<p dir=\"auto\">Run <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl get deployments -n applications</code></p>\n<p dir=\"auto\">You'll see output like the following. In this example case, both our frontend and backend deployments are unhealthy.</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nsample-app-backend-prod    0/1     1            0           42m\nsample-app-frontend-prod   0/1     1            0           42m\"><pre class=\"notranslate\"><code class=\"notranslate\">NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nsample-app-backend-prod    0/1     1            0           42m\nsample-app-frontend-prod   0/1     1            0           42m\n</code></pre></div>\n<p dir=\"auto\">We can look for more information as to why by describing our deployments next.</p>\n<p dir=\"auto\">Run <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl describe deployments sample-app-backend-prod -n applications</code></p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"Name:                   sample-app-backend-prod\nNamespace:              applications\nCreationTimestamp:      Thu, 07 Apr 2022 11:41:37 -0400\nLabels:                 app.kubernetes.io/instance=sample-app-backend-prod\n                        app.kubernetes.io/managed-by=Helm\n                        app.kubernetes.io/name=sample-app-backend-prod\n                        helm.sh/chart=k8s-service-v0.2.12\nAnnotations:            deployment.kubernetes.io/revision: 1\n                        meta.helm.sh/release-name: sample-app-backend-prod\n                        meta.helm.sh/release-namespace: applications\nSelector:               app.kubernetes.io/instance=sample-app-backend-prod,app.kubernetes.io/name=sample-app-backend-prod,gruntwork.io/deployment-type=main\nReplicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:           app.kubernetes.io/instance=sample-app-backend-prod\n                    app.kubernetes.io/name=sample-app-backend-prod\n                    gruntwork.io/deployment-type=main\n  Service Account:  gruntwork-sample-app-backend\n  Containers:\n   sample-app-backend-prod:\n    Image:       gruntwork/aws-sample-app:v0.0.4\n    Ports:       8443/TCP, 8443/TCP, 8443/TCP\n    Host Ports:  0/TCP, 0/TCP, 0/TCP\n    Liveness:    http-get https://:8443/health delay=15s timeout=1s period=30s #success=1 #failure=3\n    Readiness:   http-get https://:8443/greeting delay=15s timeout=1s period=30s #success=1 #failure=3\n    Environment:\n      CONFIG_APP_ENVIRONMENT_NAME:            prod\n      CONFIG_APP_NAME:                        backend\n      CONFIG_DATABASE_HOST:                   database\n      CONFIG_DATABASE_POOL_SIZE:              10\n      CONFIG_DATABASE_RUN_SCHEMA_MIGRATIONS:  true\n      CONFIG_SECRETS_DIR:                     /mnt/secrets\n      CONFIG_SECRETS_SECRETS_MANAGER_DB_ID:   arn:aws:secretsmanager:us-east-2:226340335990:secret:RDSDBConfig-hECb4Z\n      CONFIG_SECRETS_SECRETS_MANAGER_REGION:  us-east-2\n      CONFIG_SECRETS_SECRETS_MANAGER_TLS_ID:  arn:aws:secretsmanager:us-east-2:226340335990:secret:SampleAppBackEndCA-ALpZCe\n    Mounts:\n      /mnt/secrets/backend-secrets from secrets-manager-scratch (rw)\n  Volumes:\n   secrets-manager-scratch:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Progressing    True    NewReplicaSetAvailable\n  Available      False   MinimumReplicasUnavailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   sample-app-backend-prod-577fc88dbd (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set sample-app-backend-prod-577fc88dbd to 1\n \"><pre class=\"notranslate\"><code class=\"notranslate\">Name:                   sample-app-backend-prod\nNamespace:              applications\nCreationTimestamp:      Thu, 07 Apr 2022 11:41:37 -0400\nLabels:                 app.kubernetes.io/instance=sample-app-backend-prod\n                        app.kubernetes.io/managed-by=Helm\n                        app.kubernetes.io/name=sample-app-backend-prod\n                        helm.sh/chart=k8s-service-v0.2.12\nAnnotations:            deployment.kubernetes.io/revision: 1\n                        meta.helm.sh/release-name: sample-app-backend-prod\n                        meta.helm.sh/release-namespace: applications\nSelector:               app.kubernetes.io/instance=sample-app-backend-prod,app.kubernetes.io/name=sample-app-backend-prod,gruntwork.io/deployment-type=main\nReplicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:           app.kubernetes.io/instance=sample-app-backend-prod\n                    app.kubernetes.io/name=sample-app-backend-prod\n                    gruntwork.io/deployment-type=main\n  Service Account:  gruntwork-sample-app-backend\n  Containers:\n   sample-app-backend-prod:\n    Image:       gruntwork/aws-sample-app:v0.0.4\n    Ports:       8443/TCP, 8443/TCP, 8443/TCP\n    Host Ports:  0/TCP, 0/TCP, 0/TCP\n    Liveness:    http-get https://:8443/health delay=15s timeout=1s period=30s #success=1 #failure=3\n    Readiness:   http-get https://:8443/greeting delay=15s timeout=1s period=30s #success=1 #failure=3\n    Environment:\n      CONFIG_APP_ENVIRONMENT_NAME:            prod\n      CONFIG_APP_NAME:                        backend\n      CONFIG_DATABASE_HOST:                   database\n      CONFIG_DATABASE_POOL_SIZE:              10\n      CONFIG_DATABASE_RUN_SCHEMA_MIGRATIONS:  true\n      CONFIG_SECRETS_DIR:                     /mnt/secrets\n      CONFIG_SECRETS_SECRETS_MANAGER_DB_ID:   arn:aws:secretsmanager:us-east-2:226340335990:secret:RDSDBConfig-hECb4Z\n      CONFIG_SECRETS_SECRETS_MANAGER_REGION:  us-east-2\n      CONFIG_SECRETS_SECRETS_MANAGER_TLS_ID:  arn:aws:secretsmanager:us-east-2:226340335990:secret:SampleAppBackEndCA-ALpZCe\n    Mounts:\n      /mnt/secrets/backend-secrets from secrets-manager-scratch (rw)\n  Volumes:\n   secrets-manager-scratch:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Progressing    True    NewReplicaSetAvailable\n  Available      False   MinimumReplicasUnavailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   sample-app-backend-prod-577fc88dbd (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set sample-app-backend-prod-577fc88dbd to 1\n \n</code></pre></div>\n<p dir=\"auto\">This confirms that our app is not available because we've not reached the desired number of replicas.</p>\n<p dir=\"auto\">Let's step one level deeper and inpsect the nodes making up our EKS cluster, keeping in mind that, since our Ref Arch was just deployed, these nodes may be backed by Fargate:</p>\n<h1 dir=\"auto\">3. Inspect your EKS cluster nodes</h1>\n<p dir=\"auto\">Run <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl get nodes</code></p>\n<p dir=\"auto\">You'll see output similar to the following:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"NAME                                                   STATUS     ROLES    AGE     VERSION\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready      &lt;none&gt;   76m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready      &lt;none&gt;   76m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-47.us-east-2.compute.internal     Ready      &lt;none&gt;   79m     v1.21.2-eks-06eac09\nip-11-103-103-113.us-east-2.compute.internal           NotReady   &lt;none&gt;   79m     v1.21.5-eks-9017834\nip-11-102-83-84.us-east-2.compute.internal             NotReady   &lt;none&gt;   79m     v1.21.5-eks-9017834\nip-11-103-92-3.us-east-2.compute.internal              Ready      &lt;none&gt;   2m17s   v1.21.5-eks-9017834\"><pre class=\"notranslate\"><code class=\"notranslate\">NAME                                                   STATUS     ROLES    AGE     VERSION\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready      &lt;none&gt;   76m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready      &lt;none&gt;   76m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-47.us-east-2.compute.internal     Ready      &lt;none&gt;   79m     v1.21.2-eks-06eac09\nip-11-103-103-113.us-east-2.compute.internal           NotReady   &lt;none&gt;   79m     v1.21.5-eks-9017834\nip-11-102-83-84.us-east-2.compute.internal             NotReady   &lt;none&gt;   79m     v1.21.5-eks-9017834\nip-11-103-92-3.us-east-2.compute.internal              Ready      &lt;none&gt;   2m17s   v1.21.5-eks-9017834\n</code></pre></div>\n<p dir=\"auto\">Note the two unhealthy nodes, with status <code class=\"notranslate\">NotReady</code>. You could also verify this in the AWS web console. In this particular case the nodes were unhealthy due to an underlying hardware failure on AWS's side. This leads to the kubelet on the node being unable to report its status, including its memory and CPU usage. When this happens, EKS eventually marks the Fargate node's status as <code class=\"notranslate\">unknown</code>, leading to issues scheduling new healthy pods on the unknown nodes.</p>\n<p dir=\"auto\">Therefore, we can drain and delete these unhealthy nodes. This will allow EKS to detect their becoming unavailable and automatically reconcile them by launching new, hopefully healthy, nodes to replace them!</p>\n<h1 dir=\"auto\">4. Drain and delete unhealthy nodes</h1>\n<p dir=\"auto\">First, we'll want to drain the nodes, which safely evicts all pods from the node and makes the node ready for maintenance or deletion:</p>\n<p dir=\"auto\">Run <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl drain ip-11-103-103-113.us-east-2.compute.internal</code></p>\n<p dir=\"auto\">Next, delete the node.</p>\n<p dir=\"auto\">Run <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl delete ip-11-103-103-113.us-east-2.compute.internal</code></p>\n<p dir=\"auto\">Repeat the above two steps of draining and deleting the node for the second unhealthy node.</p>\n<p dir=\"auto\">Once this is complete, EKS should kick in and provision two new healthy nodes to compensate for the unhealthy ones you just cleaned up.</p>\n<p dir=\"auto\">You can confirm this by running <code class=\"notranslate\">aws-vault exec &lt;your-prod-aws-vault-profile&gt; -- kubectl get nodes</code>. You should see output similar to the following:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"NAME                                                   STATUS   ROLES    AGE     VERSION\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready    &lt;none&gt;   81m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready    &lt;none&gt;   81m     v1.21.2-eks-06eac09\nfargate-ip-11-103-87-47.us-east-2.compute.internal     Ready    &lt;none&gt;   84m     v1.21.2-eks-06eac09\nfargate-ip-11-103-94-175.us-east-2.compute.internal    Ready    &lt;none&gt;   55m     v1.21.2-eks-06eac09\nfargate-ip-11-103-95-219.us-east-2.compute.internal    Ready    &lt;none&gt;   84m     v1.21.2-eks-06eac09\nip-10-102-92-3.us-east-2.compute.internal              Ready    &lt;none&gt;   7m49s   v1.21.5-eks-9017834\"><pre class=\"notranslate\"><code class=\"notranslate\">NAME                                                   STATUS   ROLES    AGE     VERSION\nfargate-ip-11-103-103-165.us-east-2.compute.internal   Ready    &lt;none&gt;   81m     v1.21.2-eks-06eac09\nfargate-ip-11-103-103-193.us-east-2.compute.internal   Ready    &lt;none&gt;   81m     v1.21.2-eks-06eac09\nfargate-ip-11-103-87-47.us-east-2.compute.internal     Ready    &lt;none&gt;   84m     v1.21.2-eks-06eac09\nfargate-ip-11-103-94-175.us-east-2.compute.internal    Ready    &lt;none&gt;   55m     v1.21.2-eks-06eac09\nfargate-ip-11-103-95-219.us-east-2.compute.internal    Ready    &lt;none&gt;   84m     v1.21.2-eks-06eac09\nip-10-102-92-3.us-east-2.compute.internal              Ready    &lt;none&gt;   7m49s   v1.21.5-eks-9017834\n</code></pre></div>\n<p dir=\"auto\">With new healthy nodes available, EKS should have been able to schedule your sample app pods on these new nodes, so things should be starting to recover.</p>\n<p dir=\"auto\">At this point, you can re-check your <a href=\"https://github.com/gruntwork-io/aws-sample-app\">AWS Sample App</a> and it should be healthy again!</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://user-images.githubusercontent.com/1769996/162277046-2a4da02f-41a4-4b94-9ff6-a3ec44f3ab6a.png\"><img src=\"https://user-images.githubusercontent.com/1769996/162277046-2a4da02f-41a4-4b94-9ff6-a3ec44f3ab6a.png\" alt=\"sample-app-healthy\" style=\"max-width: 100%;\"></a></p>"}}} />

</CenterLayout>

<!-- ##DOCS-SOURCER-START
{
  "sourcePlugin": "github-discussions",
  "hash": "a3bf191abd75a0951f82767b728e6211"
}
##DOCS-SOURCER-END -->
