---
hide_table_of_contents: true
hide_title: true
custom_edit_url: null
---

import CenterLayout from "/src/components/CenterLayout"
import GitHub from "/src/components/GitHub"

<head>
  <link rel="canonical" href="https://github.com/gruntwork-io/knowledge-base/discussions/106" />
</head>

<CenterLayout>
<span className="searchCategory">Knowledge Base</span>
<h1>Deploying EKS control plane to mgmt VPC or app VPC</h1>
<GitHub discussion={{"id":"D_kwDOF8slf84AOVeq","number":106,"author":{"login":"NathanielWroblewski"},"title":"Deploying EKS control plane to mgmt VPC or app VPC","body":"In following [the guide](https://docs.gruntwork.io/docs/guides/build-it-yourself/kubernetes-cluster/deployment-walkthrough/configure-the-control-plane) for EKS deployment, it was unclear initially if the control plane was deployed to the app VPC or the management VPC because the management VPC peering had just been set-up.  It seems the management VPC is not really used in the guide, and I think this contributed to my confusion, but I've since placed a bastion there.\r\n\r\nThe docs are missing a step for establishing peering prior to adding the DNS resolver.  (Establishing peering is omitted entirely, but the DNS resolver cannot be established due to errors on the SGs for being in different VPCs).\r\n\r\nI had an issue initially where the control plane terraform never seems to finish:\r\n\r\n```\r\nodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m20s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m30s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m40s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m50s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m0s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m10s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m20s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m30s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m40s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m50s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m0s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m10s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m20s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m30s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m40s elapsed]\r\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"Error retrieiving info from endpoint: Head \\\"https://REDACTED.eks.amazonaws.com\\\": dial tcp REDACTED:443: connect: operation timed out\" name=kubergrunt\r\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"Marking api server as not ready\" name=kubergrunt\r\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"EKS cluster arn:aws:eks:REDACTED:cluster/REDACTED Kubernetes api server is not active yet\" name=kubergrunt\r\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=info msg=\"Waiting for 15s...\" name=kubergrunt\r\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m50s elapsed]\r\n```\r\n\r\nI needed to install `kubergrunt`.  Having done that, I still get issues:\r\n\r\n```\r\nERROR: Get\r\n│ \"https://REDACTED.eks.amazonaws.com/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy\":\r\n│ dial tcp REDACTED:443: i/o timeout\r\n```\r\n\r\nThe issue here is that the guide says to make the API endpoint private, but public is required by the templates to terraform the cluster.\r\n\r\nI have completed the guide, but our nodes are not registered by the cluster.  We're a little disappointed by how much the documentation and guides have diverged from the recent modules, and while we've been able to figure things out, there's been a significant time investment to get everything working properly.  The above are just some of the issues we've run into.  It'd be very helpful to keep the docs and guides in sync with the modules.\r\n\r\n```\r\n[] INFO[2021-12-27T21:45:51-05:00] Not all nodes are registered yet              name=kubergrunt\r\n[] INFO[2021-12-27T21:45:51-05:00] Waiting for 15s...                            name=kubergrunt\r\n[] INFO[2021-12-27T21:46:06-05:00] Checking if nodes ready                       name=kubergrunt\r\n[] INFO[2021-12-27T21:46:06-05:00] Not all nodes are registered yet              name=kubergrunt\r\n[] INFO[2021-12-27T21:46:06-05:00] Waiting for 15s...                            name=kubergrunt\r\n[] INFO[2021-12-27T21:46:21-05:00] Checking if nodes ready                       name=kubergrunt\r\n[] INFO[2021-12-27T21:46:21-05:00] Not all nodes are registered yet              name=kubergrunt\r\n[] INFO[2021-12-27T21:46:21-05:00] Waiting for 15s...                            name=kubergrunt\r\n[] INFO[2021-12-27T21:46:36-05:00] Checking if nodes ready                       name=kubergrunt\r\n[] INFO[2021-12-27T21:46:36-05:00] Not all nodes are registered yet              name=kubergrunt\r\n```\r\n\r\nDo we need to provision additional IAM roles and set the mapping in the cluster in order for the nodes to be registered?  Do we need to run some script?  Did the registration script which invoked in the `user-data` not run successfully?  The docs do not address these issues or how to proceed.  What should we be checking for node registration issues?\r\n\r\nr:terraform-aws-eks","bodyHTML":"<p dir=\"auto\">In following <a href=\"https://docs.gruntwork.io/docs/guides/build-it-yourself/kubernetes-cluster/deployment-walkthrough/configure-the-control-plane\" rel=\"nofollow\">the guide</a> for EKS deployment, it was unclear initially if the control plane was deployed to the app VPC or the management VPC because the management VPC peering had just been set-up.  It seems the management VPC is not really used in the guide, and I think this contributed to my confusion, but I've since placed a bastion there.</p>\n<p dir=\"auto\">The docs are missing a step for establishing peering prior to adding the DNS resolver.  (Establishing peering is omitted entirely, but the DNS resolver cannot be established due to errors on the SGs for being in different VPCs).</p>\n<p dir=\"auto\">I had an issue initially where the control plane terraform never seems to finish:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"odule.eks_cluster.null_resource.wait_for_api: Still creating... [19m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m50s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m0s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m10s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m50s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m0s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m10s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=&quot;2021-12-21T14:45:48-05:00&quot; level=warning msg=&quot;Error retrieiving info from endpoint: Head \\&quot;https://REDACTED.eks.amazonaws.com\\&quot;: dial tcp REDACTED:443: connect: operation timed out&quot; name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=&quot;2021-12-21T14:45:48-05:00&quot; level=warning msg=&quot;Marking api server as not ready&quot; name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=&quot;2021-12-21T14:45:48-05:00&quot; level=warning msg=&quot;EKS cluster arn:aws:eks:REDACTED:cluster/REDACTED Kubernetes api server is not active yet&quot; name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=&quot;2021-12-21T14:45:48-05:00&quot; level=info msg=&quot;Waiting for 15s...&quot; name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m50s elapsed]\"><pre class=\"notranslate\"><code>odule.eks_cluster.null_resource.wait_for_api: Still creating... [19m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [19m50s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m0s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m10s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [20m50s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m0s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m10s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m20s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m30s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m40s elapsed]\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"Error retrieiving info from endpoint: Head \\\"https://REDACTED.eks.amazonaws.com\\\": dial tcp REDACTED:443: connect: operation timed out\" name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"Marking api server as not ready\" name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=warning msg=\"EKS cluster arn:aws:eks:REDACTED:cluster/REDACTED Kubernetes api server is not active yet\" name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api (local-exec): [] time=\"2021-12-21T14:45:48-05:00\" level=info msg=\"Waiting for 15s...\" name=kubergrunt\nmodule.eks_cluster.null_resource.wait_for_api: Still creating... [21m50s elapsed]\n</code></pre></div>\n<p dir=\"auto\">I needed to install <code class=\"notranslate\">kubergrunt</code>.  Having done that, I still get issues:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"ERROR: Get\n│ &quot;https://REDACTED.eks.amazonaws.com/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy&quot;:\n│ dial tcp REDACTED:443: i/o timeout\"><pre class=\"notranslate\"><code>ERROR: Get\n│ \"https://REDACTED.eks.amazonaws.com/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy\":\n│ dial tcp REDACTED:443: i/o timeout\n</code></pre></div>\n<p dir=\"auto\">The issue here is that the guide says to make the API endpoint private, but public is required by the templates to terraform the cluster.</p>\n<p dir=\"auto\">I have completed the guide, but our nodes are not registered by the cluster.  We're a little disappointed by how much the documentation and guides have diverged from the recent modules, and while we've been able to figure things out, there's been a significant time investment to get everything working properly.  The above are just some of the issues we've run into.  It'd be very helpful to keep the docs and guides in sync with the modules.</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"[] INFO[2021-12-27T21:45:51-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:45:51-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:36-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:36-05:00] Not all nodes are registered yet              name=kubergrunt\"><pre class=\"notranslate\"><code>[] INFO[2021-12-27T21:45:51-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:45:51-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:46:06-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Not all nodes are registered yet              name=kubergrunt\n[] INFO[2021-12-27T21:46:21-05:00] Waiting for 15s...                            name=kubergrunt\n[] INFO[2021-12-27T21:46:36-05:00] Checking if nodes ready                       name=kubergrunt\n[] INFO[2021-12-27T21:46:36-05:00] Not all nodes are registered yet              name=kubergrunt\n</code></pre></div>\n<p dir=\"auto\">Do we need to provision additional IAM roles and set the mapping in the cluster in order for the nodes to be registered?  Do we need to run some script?  Did the registration script which invoked in the <code class=\"notranslate\">user-data</code> not run successfully?  The docs do not address these issues or how to proceed.  What should we be checking for node registration issues?</p>\n<p dir=\"auto\">r:terraform-aws-eks</p>","answer":{"body":"Hello, apologies for the frustration and challenges with using the guide. We are aware of how out of date the guide is and are intending on overhauling both the guide contents and process to ensure that they stay up to date.\r\n\r\nRegarding the issues with node registration, you should not need to do anything beyond making sure the worker ASG IAM roles are included in the `eks_worker_iam_role_arns` attribute for the call to the `eks-k8s-role-mapping` module. I suspect there were some issues with the IAM role mapping creation when you ran into issues with the private API endpoint setup. I would check the following things to troubleshoot this issue:\r\n\r\n- Introspect the `aws-auth` ConfigMap to make sure it has the worker IAM role in the configuration. You can use `kubectl` to retrieve the config map directly form the cluster: `kubectl describe configmap aws-auth -n kube-system`.\r\n- If the ConfigMap is correct, then SSH into the running nodes and introspect the `kubelet` logs for more info. You should be able to find the error logs in either syslog,`/var/log/messages` (e.g., try running `sudo tail /var/log/messages | grep kubelet`). This should give you some insights into what might be causing the issue.\r\n\r\n---\r\n\r\nIf you still have issues with deploying using the guide, you can try provisioning the cluster using an alternative approach. A recommended alternative to the guide is using [our Service Catalog module](https://github.com/gruntwork-io/terraform-aws-service-catalog/tree/master/modules/services/eks-cluster). The Service Catalog module has less configuration freedom as you are relying on prebuilt `infrastructure-modules` modules, but may work better as a starting point.\r\n\r\nYou can deploy using the Service Catalog by doing the following:\r\n\r\n1. Build the AMI using the provided [packer template](https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/eks-workers/eks-node-al2.pkr.hcl). To do so, git clone the service catalog repo and run `cd modules/services/eks-workers && packer build -var=\"version_tag=v0.68.7\" -var=\"service_catalog_ref=v0.68.7\" -var=\"aws_region=YOUR_AWS_REGION\" eks-node-al2.pkr.hcl`. Note that you may want to pass in additional `-var` inputs depending on your needs.\r\n1. Use the following updated terragrunt config, with all the `<>` variables updated to the real values for your environment:\r\n```\r\nterraform {\r\n  source = \"git@github.com/gruntwork-io/terraform-aws-service-catalog.git//modules/services/eks-cluster?ref=v0.68.7\"\r\n}\r\n\r\ninclude {\r\n  path = find_in_parent_folders()\r\n}\r\n\r\ngenerate \"provider\" {\r\n  path      = \"provider.tf\"\r\n  if_exists = \"overwrite_terragrunt\"\r\n  contents  = <<EOF\r\nprovider \"aws\" {\r\n  region = \"<YOUR_AWS_REGION>\"\r\n}\r\nEOF\r\n}\r\n\r\ninputs = {\r\n  cluster_name                  = \"eks-stage\"\r\n  cluster_instance_keypair_name = \"stage-services-us-east-1-v1\"\r\n  \r\n  vpc_id = \"<APP_VPC_ID>\"\r\n  control_plane_vpc_subnet_ids = [\"<LIST_OF_PRIVATE_APP_SUBNET_IDS>\"]\r\n  allow_inbound_api_access_from_cidr_blocks = [\"0.0.0.0/0\"]\r\n  allow_private_api_access_from_cidr_blocks = [\r\n    \"<CIDR_BLOCK_OF_APP_VPC>\",\r\n    \"<CIDR_BLOCK_OF_MGMT_VPC>\",\r\n  ]\r\n  endpoint_public_access = true  # Set to false for private API\r\n\r\n  # Fill in the ID of the AMI you built from your Packer template\r\n  cluster_instance_ami          = \"<AMI_ID>\"\r\n\r\n  # Set the max size to double the min size so the extra capacity can be used to do a zero-downtime deployment of updates\r\n  # to the EKS Cluster Nodes (e.g. when you update the AMI). For docs on how to roll out updates to the cluster, see:\r\n  # https://github.com/gruntwork-io/terraform-aws-eks/tree/master/modules/eks-cluster-workers#how-do-i-roll-out-an-update-to-the-instances\r\n  autoscaling_group_configurations = {\r\n    asg = {\r\n      min_size      = 3\r\n      max_size      = 6\r\n      asg_instance_type = \"t2.small\"\r\n      subnet_ids = [\"<LIST_OF_PRIVATE_APP_SUBNET_IDS>\"]\r\n    }\r\n  }\r\n\r\n  # If your IAM users are defined in a separate AWS account (e.g., in a security account), pass in the ARN of an IAM\r\n  # role in that account that ssh-grunt on the worker nodes can assume to look up IAM group membership and public SSH\r\n  # keys\r\n  external_account_ssh_grunt_role_arn = \"arn:aws:iam::1111222233333:role/allow-ssh-grunt-access-from-other-accounts\"\r\n\r\n  # Configure your role mappings\r\n  iam_role_to_rbac_group_mappings = {\r\n    # Give anyone using the full-access IAM role admin permissions\r\n    \"arn:aws:iam::444444444444:role/allow-full-access-from-other-accounts\" = [\"system:masters\"]\r\n\r\n    # Give anyone using the developers IAM role developer permissions. Kubernetes will automatically create this group\r\n    # if it doesn't exist already, but you're still responsible for binding permissions to it!\r\n    \"arn:aws:iam::444444444444:role/allow-dev-access-from-other-accounts\" = [\"developers\"]\r\n  }\r\n}\r\n```\r\n\r\nNote that like the guide, you will want to deploy using `endpoint_public_access = true` first, and then switching to `endpoint_public_access = false` due to the network access issues you ran into. Alternatively, you can deploy through a VPN connection that allows you to VPN into the mgmt VPC.\r\n\r\n---\r\n\r\nSide note: I believe you can deploy the VPC without the DNS resolvers now. This used to be a requirement for accessing the Kubernetes API endpoint on EKS clusters with private access over a VPC peer, but as far as I know, AWS has since updated the networking infrastructure to no longer need it. The only reason I mention it is because the DNS resolvers can add up to be quite pricey (approximately $500/month), so you may want to consider removing it if you are tight on budget.\r\n\r\nAlternatively, you can consider omitting the mgmt VPC altogether and deploy the bastion/VPN server into the app VPC in the public network space. The `mgmt` VPC architecture is most useful/recommended if you intend on having more than one VPC for your applications. Otherwise, it can be unnecessary overhead. It is fairly straightforward to introduce one after the fact as well, so you may want to consider a single VPC architecture if you don't have the networking needs.","bodyHTML":"<p dir=\"auto\">Hello, apologies for the frustration and challenges with using the guide. We are aware of how out of date the guide is and are intending on overhauling both the guide contents and process to ensure that they stay up to date.</p>\n<p dir=\"auto\">Regarding the issues with node registration, you should not need to do anything beyond making sure the worker ASG IAM roles are included in the <code class=\"notranslate\">eks_worker_iam_role_arns</code> attribute for the call to the <code class=\"notranslate\">eks-k8s-role-mapping</code> module. I suspect there were some issues with the IAM role mapping creation when you ran into issues with the private API endpoint setup. I would check the following things to troubleshoot this issue:</p>\n<ul dir=\"auto\">\n<li>Introspect the <code class=\"notranslate\">aws-auth</code> ConfigMap to make sure it has the worker IAM role in the configuration. You can use <code class=\"notranslate\">kubectl</code> to retrieve the config map directly form the cluster: <code class=\"notranslate\">kubectl describe configmap aws-auth -n kube-system</code>.</li>\n<li>If the ConfigMap is correct, then SSH into the running nodes and introspect the <code class=\"notranslate\">kubelet</code> logs for more info. You should be able to find the error logs in either syslog,<code class=\"notranslate\">/var/log/messages</code> (e.g., try running <code class=\"notranslate\">sudo tail /var/log/messages | grep kubelet</code>). This should give you some insights into what might be causing the issue.</li>\n</ul>\n<hr>\n<p dir=\"auto\">If you still have issues with deploying using the guide, you can try provisioning the cluster using an alternative approach. A recommended alternative to the guide is using <a href=\"https://github.com/gruntwork-io/terraform-aws-service-catalog/tree/master/modules/services/eks-cluster\">our Service Catalog module</a>. The Service Catalog module has less configuration freedom as you are relying on prebuilt <code class=\"notranslate\">infrastructure-modules</code> modules, but may work better as a starting point.</p>\n<p dir=\"auto\">You can deploy using the Service Catalog by doing the following:</p>\n<ol dir=\"auto\">\n<li>Build the AMI using the provided <a href=\"https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/eks-workers/eks-node-al2.pkr.hcl\">packer template</a>. To do so, git clone the service catalog repo and run <code class=\"notranslate\">cd modules/services/eks-workers &amp;&amp; packer build -var=\"version_tag=v0.68.7\" -var=\"service_catalog_ref=v0.68.7\" -var=\"aws_region=YOUR_AWS_REGION\" eks-node-al2.pkr.hcl</code>. Note that you may want to pass in additional <code class=\"notranslate\">-var</code> inputs depending on your needs.</li>\n<li>Use the following updated terragrunt config, with all the <code class=\"notranslate\">&lt;&gt;</code> variables updated to the real values for your environment:</li>\n</ol>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"terraform {\n  source = &quot;git@github.com/gruntwork-io/terraform-aws-service-catalog.git//modules/services/eks-cluster?ref=v0.68.7&quot;\n}\n\ninclude {\n  path = find_in_parent_folders()\n}\n\ngenerate &quot;provider&quot; {\n  path      = &quot;provider.tf&quot;\n  if_exists = &quot;overwrite_terragrunt&quot;\n  contents  = &lt;&lt;EOF\nprovider &quot;aws&quot; {\n  region = &quot;&lt;YOUR_AWS_REGION&gt;&quot;\n}\nEOF\n}\n\ninputs = {\n  cluster_name                  = &quot;eks-stage&quot;\n  cluster_instance_keypair_name = &quot;stage-services-us-east-1-v1&quot;\n  \n  vpc_id = &quot;&lt;APP_VPC_ID&gt;&quot;\n  control_plane_vpc_subnet_ids = [&quot;&lt;LIST_OF_PRIVATE_APP_SUBNET_IDS&gt;&quot;]\n  allow_inbound_api_access_from_cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  allow_private_api_access_from_cidr_blocks = [\n    &quot;&lt;CIDR_BLOCK_OF_APP_VPC&gt;&quot;,\n    &quot;&lt;CIDR_BLOCK_OF_MGMT_VPC&gt;&quot;,\n  ]\n  endpoint_public_access = true  # Set to false for private API\n\n  # Fill in the ID of the AMI you built from your Packer template\n  cluster_instance_ami          = &quot;&lt;AMI_ID&gt;&quot;\n\n  # Set the max size to double the min size so the extra capacity can be used to do a zero-downtime deployment of updates\n  # to the EKS Cluster Nodes (e.g. when you update the AMI). For docs on how to roll out updates to the cluster, see:\n  # https://github.com/gruntwork-io/terraform-aws-eks/tree/master/modules/eks-cluster-workers#how-do-i-roll-out-an-update-to-the-instances\n  autoscaling_group_configurations = {\n    asg = {\n      min_size      = 3\n      max_size      = 6\n      asg_instance_type = &quot;t2.small&quot;\n      subnet_ids = [&quot;&lt;LIST_OF_PRIVATE_APP_SUBNET_IDS&gt;&quot;]\n    }\n  }\n\n  # If your IAM users are defined in a separate AWS account (e.g., in a security account), pass in the ARN of an IAM\n  # role in that account that ssh-grunt on the worker nodes can assume to look up IAM group membership and public SSH\n  # keys\n  external_account_ssh_grunt_role_arn = &quot;arn:aws:iam::1111222233333:role/allow-ssh-grunt-access-from-other-accounts&quot;\n\n  # Configure your role mappings\n  iam_role_to_rbac_group_mappings = {\n    # Give anyone using the full-access IAM role admin permissions\n    &quot;arn:aws:iam::444444444444:role/allow-full-access-from-other-accounts&quot; = [&quot;system:masters&quot;]\n\n    # Give anyone using the developers IAM role developer permissions. Kubernetes will automatically create this group\n    # if it doesn't exist already, but you're still responsible for binding permissions to it!\n    &quot;arn:aws:iam::444444444444:role/allow-dev-access-from-other-accounts&quot; = [&quot;developers&quot;]\n  }\n}\"><pre class=\"notranslate\"><code>terraform {\n  source = \"git@github.com/gruntwork-io/terraform-aws-service-catalog.git//modules/services/eks-cluster?ref=v0.68.7\"\n}\n\ninclude {\n  path = find_in_parent_folders()\n}\n\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents  = &lt;&lt;EOF\nprovider \"aws\" {\n  region = \"&lt;YOUR_AWS_REGION&gt;\"\n}\nEOF\n}\n\ninputs = {\n  cluster_name                  = \"eks-stage\"\n  cluster_instance_keypair_name = \"stage-services-us-east-1-v1\"\n  \n  vpc_id = \"&lt;APP_VPC_ID&gt;\"\n  control_plane_vpc_subnet_ids = [\"&lt;LIST_OF_PRIVATE_APP_SUBNET_IDS&gt;\"]\n  allow_inbound_api_access_from_cidr_blocks = [\"0.0.0.0/0\"]\n  allow_private_api_access_from_cidr_blocks = [\n    \"&lt;CIDR_BLOCK_OF_APP_VPC&gt;\",\n    \"&lt;CIDR_BLOCK_OF_MGMT_VPC&gt;\",\n  ]\n  endpoint_public_access = true  # Set to false for private API\n\n  # Fill in the ID of the AMI you built from your Packer template\n  cluster_instance_ami          = \"&lt;AMI_ID&gt;\"\n\n  # Set the max size to double the min size so the extra capacity can be used to do a zero-downtime deployment of updates\n  # to the EKS Cluster Nodes (e.g. when you update the AMI). For docs on how to roll out updates to the cluster, see:\n  # https://github.com/gruntwork-io/terraform-aws-eks/tree/master/modules/eks-cluster-workers#how-do-i-roll-out-an-update-to-the-instances\n  autoscaling_group_configurations = {\n    asg = {\n      min_size      = 3\n      max_size      = 6\n      asg_instance_type = \"t2.small\"\n      subnet_ids = [\"&lt;LIST_OF_PRIVATE_APP_SUBNET_IDS&gt;\"]\n    }\n  }\n\n  # If your IAM users are defined in a separate AWS account (e.g., in a security account), pass in the ARN of an IAM\n  # role in that account that ssh-grunt on the worker nodes can assume to look up IAM group membership and public SSH\n  # keys\n  external_account_ssh_grunt_role_arn = \"arn:aws:iam::1111222233333:role/allow-ssh-grunt-access-from-other-accounts\"\n\n  # Configure your role mappings\n  iam_role_to_rbac_group_mappings = {\n    # Give anyone using the full-access IAM role admin permissions\n    \"arn:aws:iam::444444444444:role/allow-full-access-from-other-accounts\" = [\"system:masters\"]\n\n    # Give anyone using the developers IAM role developer permissions. Kubernetes will automatically create this group\n    # if it doesn't exist already, but you're still responsible for binding permissions to it!\n    \"arn:aws:iam::444444444444:role/allow-dev-access-from-other-accounts\" = [\"developers\"]\n  }\n}\n</code></pre></div>\n<p dir=\"auto\">Note that like the guide, you will want to deploy using <code class=\"notranslate\">endpoint_public_access = true</code> first, and then switching to <code class=\"notranslate\">endpoint_public_access = false</code> due to the network access issues you ran into. Alternatively, you can deploy through a VPN connection that allows you to VPN into the mgmt VPC.</p>\n<hr>\n<p dir=\"auto\">Side note: I believe you can deploy the VPC without the DNS resolvers now. This used to be a requirement for accessing the Kubernetes API endpoint on EKS clusters with private access over a VPC peer, but as far as I know, AWS has since updated the networking infrastructure to no longer need it. The only reason I mention it is because the DNS resolvers can add up to be quite pricey (approximately $500/month), so you may want to consider removing it if you are tight on budget.</p>\n<p dir=\"auto\">Alternatively, you can consider omitting the mgmt VPC altogether and deploy the bastion/VPN server into the app VPC in the public network space. The <code class=\"notranslate\">mgmt</code> VPC architecture is most useful/recommended if you intend on having more than one VPC for your applications. Otherwise, it can be unnecessary overhead. It is fairly straightforward to introduce one after the fact as well, so you may want to consider a single VPC architecture if you don't have the networking needs.</p>"}}} />

</CenterLayout>
  

<!-- ##DOCS-SOURCER-START
{
  "sourcePlugin": "github-discussions",
  "hash": "1af964eafc95f3ca504455bc83f3bc76"
}
##DOCS-SOURCER-END -->
