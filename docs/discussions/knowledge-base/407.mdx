---
hide_table_of_contents: true
hide_title: true
custom_edit_url: null
---

import CenterLayout from "/src/components/CenterLayout"
import GitHub from "/src/components/GitHub"

<CenterLayout>
<span className="searchCategory">Knowledge Base</span>
<h1>Reference Architecture: Handling CI/CD and EKS deployments</h1>
<GitHub discussion={{"id":"D_kwDOF8slf84APePo","number":407,"author":{"login":"gorkemgoknar"},"title":"Reference Architecture: Handling CI/CD and EKS deployments","body":"\nWe had reference  architecture installed already with EKS , but suspecting we will have some troubles in future.\r\n\r\n1 - Trying to understrand more on recent Infrastructure pipeline CI/CD,  we see that changes to envcommon (for all stages) and it is supposed to do apply, but if we have multiple changes in one PR (add eks node group + add changes to application like affinity for helm) and if only one fails (e.g. simple helm changes seem to take too much time and even if change is done timeout occurs and makes apply fail) we are stuck with code is being there for the architecture yet changes are not applied.\r\n\r\nI saw it was suggested to make changes as little as possible https://github.com/gruntwork-io/terragrunt/issues/720  but it does seem to fit for a fast test and deploy way as a single CI run takes at least 20 minutes (deploy runner bootup is only 2 minutes other are plan and deploy steps, timeout and waits from AWS excluded). \r\n\r\n2 - Terraform does state locking during plan and deploy , if there are two CI deployments (even independent modules) at close time done to each other (think of 2 K8s services which are updating ECR docker image at same time) one of them will likely fail.\r\nHow can we solve this issue (using Gitlab actions)? \r\nCan Deploy Runner wait for previous one to finish (it seems kind of stateless and no DB backing) \r\nAlthough may not be a big issue for 1-2 service it will be an issue once there are 10s even 50s of docker images using multiple ECR repositories.\r\n\r\n3- Relevant to 2nd question we mainly use docker images for multiple applications (it may include frontend + backend  that has public side or backend only within VPC)  and all may have different CPU/memory even latency requirements and may need  to add specific affinitys for targeting node groups.\r\nWrapping them each in a K8service limits our options as they maybe used as simple helm charts and instead of using another wrapper we may use directly yaml files.\r\nIs there a proper/suggested way to better organize multiple helm deployments (instead of relying to use override_chart_inputs  which is not guaranteed to convert to proper yaml ) ? \r\n\r\nThanks in response\r\n\n\n---\n\n<ins datetime=\"2022-05-06T09:18:37Z\">\n  <p><a href=\"https://support.gruntwork.io/hc/requests/108560\">Tracked in ticket #108560</a></p>\n</ins>\n","bodyHTML":"<p dir=\"auto\">We had reference  architecture installed already with EKS , but suspecting we will have some troubles in future.</p>\n<p dir=\"auto\">1 - Trying to understrand more on recent Infrastructure pipeline CI/CD,  we see that changes to envcommon (for all stages) and it is supposed to do apply, but if we have multiple changes in one PR (add eks node group + add changes to application like affinity for helm) and if only one fails (e.g. simple helm changes seem to take too much time and even if change is done timeout occurs and makes apply fail) we are stuck with code is being there for the architecture yet changes are not applied.</p>\n<p dir=\"auto\">I saw it was suggested to make changes as little as possible <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"451006204\" data-permission-text=\"Title is private\" data-url=\"https://github.com/gruntwork-io/terragrunt/issues/720\" data-hovercard-type=\"issue\" data-hovercard-url=\"/gruntwork-io/terragrunt/issues/720/hovercard\" href=\"https://github.com/gruntwork-io/terragrunt/issues/720\">gruntwork-io/terragrunt#720</a>  but it does seem to fit for a fast test and deploy way as a single CI run takes at least 20 minutes (deploy runner bootup is only 2 minutes other are plan and deploy steps, timeout and waits from AWS excluded).</p>\n<p dir=\"auto\">2 - Terraform does state locking during plan and deploy , if there are two CI deployments (even independent modules) at close time done to each other (think of 2 K8s services which are updating ECR docker image at same time) one of them will likely fail.<br>\nHow can we solve this issue (using Gitlab actions)?<br>\nCan Deploy Runner wait for previous one to finish (it seems kind of stateless and no DB backing)<br>\nAlthough may not be a big issue for 1-2 service it will be an issue once there are 10s even 50s of docker images using multiple ECR repositories.</p>\n<p dir=\"auto\">3- Relevant to 2nd question we mainly use docker images for multiple applications (it may include frontend + backend  that has public side or backend only within VPC)  and all may have different CPU/memory even latency requirements and may need  to add specific affinitys for targeting node groups.<br>\nWrapping them each in a K8service limits our options as they maybe used as simple helm charts and instead of using another wrapper we may use directly yaml files.<br>\nIs there a proper/suggested way to better organize multiple helm deployments (instead of relying to use override_chart_inputs  which is not guaranteed to convert to proper yaml ) ?</p>\n<p dir=\"auto\">Thanks in response</p>\n<hr>\n<ins datetime=\"2022-05-06T09:18:37Z\">\n  <p dir=\"auto\"><a href=\"https://support.gruntwork.io/hc/requests/108560\" rel=\"nofollow\">Tracked in ticket #108560</a></p>\n</ins>","answer":{"body":"> 1 - (e.g. simple helm changes seem to take too much time and even if change is done timeout occurs and makes apply fail)\r\n\r\nYou can tweak the waiting behavior of `helm` deployments using the `wait` and `wait_timeout` input variables of the `k8s-service` module: https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/k8s-service/variables.tf#L570-L582\r\n\r\nThis may help stabilize your deployments.\r\n\r\n> we are stuck with code is being there for the architecture yet changes are not applied.\r\n\r\nIf a deployment failed, the idea is that something needs to change (either the cloud, or the code). The resolution path will be different depending on the nature of the error. In this case, you probably want to retry the CI job so that it attempts to `apply` the code again to try to get to steady state.\r\n\r\nNote that you can also have `terragrunt` automatically retry on errors using the `retryable_errors` attribute in the config: https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#retryable_errors\r\n\r\nBasically, even if we skip the error, you still will have undeployed code because the `helm_release` object will be tainted in `terraform`, so at a minimum a retry is necessary.\r\n\r\n> 2 - Terraform does state locking during plan and deploy , if there are two CI deployments (even independent modules) at close time done to each other (think of 2 K8s services which are updating ECR docker image at same time) one of them will likely fail.\r\nHow can we solve this issue (using Gitlab actions)?\r\n> Can Deploy Runner wait for previous one to finish (it seems kind of stateless and no DB backing)\r\n> Although may not be a big issue for 1-2 service it will be an issue once there are 10s even 50s of docker images using multiple ECR repositories.\r\n\r\nUnfortunately we don't have the ability to bake in locking mechanisms to the ECS Deploy Runner. I filed https://github.com/gruntwork-io/terraform-aws-ci/issues/440 to track this feature request.\r\n\r\nIn the meantime, as a workaround, you can probably handle this in `terragrunt` using the `retryable_errors` mechanism mentioned above. That is, you can have terragrunt automatically retry if it gets the \"could not obtain lock\" error from terraform.\r\n\r\nWith that said, this should only an issue if you have many commits changing the same service. If you have such a situation, I think there is more risk that something will undo a change unintentionally even if you have waiting involved. I recommend rearchitecting your terragrunt code to minimize overlapping changes as much as possible.\r\n\r\n> 3- Relevant to 2nd question we mainly use docker images for multiple applications (it may include frontend + backend that has public side or backend only within VPC) and all may have different CPU/memory even latency requirements and may need to add specific affinitys for targeting node groups.\r\n> Wrapping them each in a K8service limits our options as they maybe used as simple helm charts and instead of using another wrapper we may use directly yaml files.\r\n> Is there a proper/suggested way to better organize multiple helm deployments (instead of relying to use override_chart_inputs which is not guaranteed to convert to proper yaml ) ?\r\n\r\nI am a bit confused as to what you want to accomplish here, but assuming you want to deploy the different services as a single unit, then the best approach would be to define you own service module that makes the necessary calls to `k8s-service` for each of your service.","bodyHTML":"<blockquote>\n<p dir=\"auto\">1 - (e.g. simple helm changes seem to take too much time and even if change is done timeout occurs and makes apply fail)</p>\n</blockquote>\n<p dir=\"auto\">You can tweak the waiting behavior of <code class=\"notranslate\">helm</code> deployments using the <code class=\"notranslate\">wait</code> and <code class=\"notranslate\">wait_timeout</code> input variables of the <code class=\"notranslate\">k8s-service</code> module: <a href=\"https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/k8s-service/variables.tf#L570-L582\">https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/k8s-service/variables.tf#L570-L582</a></p>\n<p dir=\"auto\">This may help stabilize your deployments.</p>\n<blockquote>\n<p dir=\"auto\">we are stuck with code is being there for the architecture yet changes are not applied.</p>\n</blockquote>\n<p dir=\"auto\">If a deployment failed, the idea is that something needs to change (either the cloud, or the code). The resolution path will be different depending on the nature of the error. In this case, you probably want to retry the CI job so that it attempts to <code class=\"notranslate\">apply</code> the code again to try to get to steady state.</p>\n<p dir=\"auto\">Note that you can also have <code class=\"notranslate\">terragrunt</code> automatically retry on errors using the <code class=\"notranslate\">retryable_errors</code> attribute in the config: <a href=\"https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#retryable_errors\" rel=\"nofollow\">https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#retryable_errors</a></p>\n<p dir=\"auto\">Basically, even if we skip the error, you still will have undeployed code because the <code class=\"notranslate\">helm_release</code> object will be tainted in <code class=\"notranslate\">terraform</code>, so at a minimum a retry is necessary.</p>\n<blockquote>\n<p dir=\"auto\">2 - Terraform does state locking during plan and deploy , if there are two CI deployments (even independent modules) at close time done to each other (think of 2 K8s services which are updating ECR docker image at same time) one of them will likely fail.<br>\nHow can we solve this issue (using Gitlab actions)?<br>\nCan Deploy Runner wait for previous one to finish (it seems kind of stateless and no DB backing)<br>\nAlthough may not be a big issue for 1-2 service it will be an issue once there are 10s even 50s of docker images using multiple ECR repositories.</p>\n</blockquote>\n<p dir=\"auto\">Unfortunately we don't have the ability to bake in locking mechanisms to the ECS Deploy Runner. I filed <a href=\"https://github.com/gruntwork-io/terraform-aws-ci/issues/440\">https://github.com/gruntwork-io/terraform-aws-ci/issues/440</a> to track this feature request.</p>\n<p dir=\"auto\">In the meantime, as a workaround, you can probably handle this in <code class=\"notranslate\">terragrunt</code> using the <code class=\"notranslate\">retryable_errors</code> mechanism mentioned above. That is, you can have terragrunt automatically retry if it gets the \"could not obtain lock\" error from terraform.</p>\n<p dir=\"auto\">With that said, this should only an issue if you have many commits changing the same service. If you have such a situation, I think there is more risk that something will undo a change unintentionally even if you have waiting involved. I recommend rearchitecting your terragrunt code to minimize overlapping changes as much as possible.</p>\n<blockquote>\n<p dir=\"auto\">3- Relevant to 2nd question we mainly use docker images for multiple applications (it may include frontend + backend that has public side or backend only within VPC) and all may have different CPU/memory even latency requirements and may need to add specific affinitys for targeting node groups.<br>\nWrapping them each in a K8service limits our options as they maybe used as simple helm charts and instead of using another wrapper we may use directly yaml files.<br>\nIs there a proper/suggested way to better organize multiple helm deployments (instead of relying to use override_chart_inputs which is not guaranteed to convert to proper yaml ) ?</p>\n</blockquote>\n<p dir=\"auto\">I am a bit confused as to what you want to accomplish here, but assuming you want to deploy the different services as a single unit, then the best approach would be to define you own service module that makes the necessary calls to <code class=\"notranslate\">k8s-service</code> for each of your service.</p>"}}} />

</CenterLayout>
  

<!-- ##DOCS-SOURCER-START
{
  "sourcePlugin": "github-discussions",
  "hash": "1d06dbb7bbb6a67bf5e9298926071299"
}
##DOCS-SOURCER-END -->
