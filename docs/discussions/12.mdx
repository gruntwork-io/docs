---
hide_table_of_contents: true
hide_title: true
custom_edit_url: null
---

import CenterLayout from "/src/components/CenterLayout"
import GitHub from "/src/components/GitHub"

<CenterLayout>
<span class="title">Knowledge Base</span>
<h1>EKS cluster exhausted all our private app subnet&apos;s IP addresses</h1>
<GitHub json={{"id":"MDEwOkRpc2N1c3Npb24zNTQxNzM0","number":12,"author":{"login":"gruntwork-support"},"title":"EKS cluster exhausted all our private app subnet's IP addresses","body":"_This message was extracted from a discussion that originally took place in Gruntwork Community Slack. Names and URLs have been removed where appropriate_\n\n**From a customer**\n\nHey Gruntwork folks! :wave:\n\nWe ran into a problem in our Dev environment today where our EKS cluster exhausted all our private app subnet's IP addresses. Fun!\n\nI was looking into this more and saw that our subnets (created with `vpc-app` module) are using a default prefix of `/21` giving them 2048 addresses each.\n\nOur CIDR uses `/16` which effectively allows for up to 65,536 total addresses, so we have some room to grow.\n\nMy question for you is how to accomplish this. I see that I can specify `private_app_subnet_cidr_blocks`, but I'd rather not have to manually calculate out each CIDR block to pass in to the module for every one of our VPCs. Is there a better way? Perhaps with the `subnet_spacing` vars and `subnet_bits` vars? I tinkered a little with that but was not really sure how to calculate the proper spacing given the desired bits that I gave it.","bodyHTML":"<p dir=\"auto\"><em>This message was extracted from a discussion that originally took place in Gruntwork Community Slack. Names and URLs have been removed where appropriate</em></p>\n<p dir=\"auto\"><strong>From a customer</strong></p>\n<p dir=\"auto\">Hey Gruntwork folks! <g-emoji class=\"g-emoji\" alias=\"wave\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png\">ðŸ‘‹</g-emoji></p>\n<p dir=\"auto\">We ran into a problem in our Dev environment today where our EKS cluster exhausted all our private app subnet's IP addresses. Fun!</p>\n<p dir=\"auto\">I was looking into this more and saw that our subnets (created with <code class=\"notranslate\">vpc-app</code> module) are using a default prefix of <code class=\"notranslate\">/21</code> giving them 2048 addresses each.</p>\n<p dir=\"auto\">Our CIDR uses <code class=\"notranslate\">/16</code> which effectively allows for up to 65,536 total addresses, so we have some room to grow.</p>\n<p dir=\"auto\">My question for you is how to accomplish this. I see that I can specify <code class=\"notranslate\">private_app_subnet_cidr_blocks</code>, but I'd rather not have to manually calculate out each CIDR block to pass in to the module for every one of our VPCs. Is there a better way? Perhaps with the <code class=\"notranslate\">subnet_spacing</code> vars and <code class=\"notranslate\">subnet_bits</code> vars? I tinkered a little with that but was not really sure how to calculate the proper spacing given the desired bits that I gave it.</p>","answer":{"body":"**From a grunt**\n\nJumping in a bit late...\n\nAFAIK, there are a few alternatives to deal with the lack of IPs issue, with varying levels of difficulty:\n\n- Add more subnets to EKS itself (I don't remember which specific configuration it is but EKS itself only use some subnets). If you create new subnets and then allocate them to be used by EKS it will use them. (Can be a band-aid solution when you still have space on your VPC and cannot migrate to a bigger one). This seems like a newer tutorial, I don't remember if we had to do all the steps, though. Maybe we used this one instead :thinking_face:\n- Use an alternate compatible CNI plugin , with these you will not be restricted by the IP limitations of the  AWS CNI. I've heard interesting things about these alternative CNIs like the fact that IPs and Pods do not end up being a 1:1 scenario, but as I've never used them I don't know how they work nor how complex/challenging they are to configure :disappointed:\n- I couldn't find, but I remember reading/watching some presentation about the \"million pods club\" where there was some debates about how to reach astounding numbers of pods in EKS","bodyHTML":"<p dir=\"auto\"><strong>From a grunt</strong></p>\n<p dir=\"auto\">Jumping in a bit late...</p>\n<p dir=\"auto\">AFAIK, there are a few alternatives to deal with the lack of IPs issue, with varying levels of difficulty:</p>\n<ul dir=\"auto\">\n<li>Add more subnets to EKS itself (I don't remember which specific configuration it is but EKS itself only use some subnets). If you create new subnets and then allocate them to be used by EKS it will use them. (Can be a band-aid solution when you still have space on your VPC and cannot migrate to a bigger one). This seems like a newer tutorial, I don't remember if we had to do all the steps, though. Maybe we used this one instead :thinking_face:</li>\n<li>Use an alternate compatible CNI plugin , with these you will not be restricted by the IP limitations of the  AWS CNI. I've heard interesting things about these alternative CNIs like the fact that IPs and Pods do not end up being a 1:1 scenario, but as I've never used them I don't know how they work nor how complex/challenging they are to configure <g-emoji class=\"g-emoji\" alias=\"disappointed\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f61e.png\">ðŸ˜ž</g-emoji></li>\n<li>I couldn't find, but I remember reading/watching some presentation about the \"million pods club\" where there was some debates about how to reach astounding numbers of pods in EKS</li>\n</ul>"},"comments":{"edges":[{"node":{"author":{"login":"gruntwork-support"},"body":"**From a grunt**\n\nHello! I'm not the perfect person to answer this as I'm fairly new to calculating CIDR subnets, but I found Terraform's native functions `cidrsubnet` (doc) and `cidrsubnets` (doc) super handy.\nYep, you would use `subnet_spacing` and `subnet_bits.` To see exactly what you would get, run them in terraform console, for example:\n```\n% terraform console\n> cidrsubnet(\"10.10.0.0/16\", 4, 10)\n10.10.160.0/20\n```\nHere, the subnet_bits are 4, subnet_spacing is 10. (Hashicorp calls these newbits and netnum.)\nBut cidrsubnets will give you a whole set of them\n\n```\n> cidrsubnets(\"10.1.0.0/16\", 4, 4, 8, 4)\n[\n  \"10.1.0.0/20\",\n  \"10.1.16.0/20\",\n  \"10.1.32.0/24\",\n  \"10.1.48.0/20\",\n]\n```\n\nYeah, super fun when you run out of IPs for the EKS cluster. What size instances are you running, out of curiosity?"}},{"node":{"author":{"login":"gruntwork-support"},"body":"**From a grunt**\n\nIn case you're interested, there's this very arcane documentation about how many ENIs you can have per instance type. Hat tip to person who pointed this out. https://github.com/aws/amazon-vpc-cni-k8s/blob/88d17816c1e07dbdb03a546f1966574ab14356f1/misc/eni-max-pods.txt"}},{"node":{"author":{"login":"gruntwork-support"},"body":"**From a grunt**\n\nJumping in a bit late...\n\nAFAIK, there are a few alternatives to deal with the lack of IPs issue, with varying levels of difficulty:\n\n- Add more subnets to EKS itself (I don't remember which specific configuration it is but EKS itself only use some subnets). If you create new subnets and then allocate them to be used by EKS it will use them. (Can be a band-aid solution when you still have space on your VPC and cannot migrate to a bigger one). This seems like a newer tutorial, I don't remember if we had to do all the steps, though. Maybe we used this one instead :thinking_face:\n- Use an alternate compatible CNI plugin , with these you will not be restricted by the IP limitations of the  AWS CNI. I've heard interesting things about these alternative CNIs like the fact that IPs and Pods do not end up being a 1:1 scenario, but as I've never used them I don't know how they work nor how complex/challenging they are to configure :disappointed:\n- I couldn't find, but I remember reading/watching some presentation about the \"million pods club\" where there was some debates about how to reach astounding numbers of pods in EKS"}}]}}} />

</CenterLayout>
  

<!-- ##DOCS-SOURCER-START
{
  "sourcePlugin": "github-discussions",
  "hash": "6db2e3311348508df8c8d017d1d391c7"
}
##DOCS-SOURCER-END -->
